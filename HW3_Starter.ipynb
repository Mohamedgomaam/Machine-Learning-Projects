{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn.linear_model\n",
    "import sklearn.tree\n",
    "import sklearn.metrics\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starter code students need to edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_TP_TN_FP_FN(ytrue_N, yhat_N):\n",
    "    ''' Compute counts of four possible outcomes of a binary classifier for evaluation.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    ytrue_N : 1D array of floats\n",
    "        Each entry represents the binary value (0 or 1) of 'true' label of one example\n",
    "        One entry per example in current dataset\n",
    "    yhat_N : 1D array of floats\n",
    "        Each entry represents a predicted binary value (either 0 or 1).\n",
    "        One entry per example in current dataset.\n",
    "        Needs to be same size as ytrue_N.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TP : float\n",
    "        Number of true positives\n",
    "    TN : float\n",
    "        Number of true negatives\n",
    "    FP : float\n",
    "        Number of false positives\n",
    "    FN : float\n",
    "        Number of false negatives\n",
    "    '''\n",
    "    TP = 0.0\n",
    "    TN = 0.0\n",
    "    FP = 0.0\n",
    "    FN = 0.0\n",
    "    for i in range(ytrue_N):\n",
    "      if yhat_N[i]==0 and ytrue_N[i]==0:\n",
    "        TN +=1\n",
    "      elif yhat_N[i]==1 and ytrue_N[i]==0:\n",
    "        FP+=1\n",
    "      elif yhat_N[i]==0 and ytrue_N[i]==1:\n",
    "        FN+=1\n",
    "      elif yhat_N[i]==1 and ytrue_N[i]==1:\n",
    "        TP+=1\n",
    "    return TP, TN, FP, FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starter code that should be used as is.\n",
    "\n",
    "No need to edit these functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_perf_metrics_for_threshold(ytrue_N, yproba1_N, thresh):\n",
    "    ''' Compute performance metrics for a given probabilistic classifier and threshold\n",
    "    '''\n",
    "    tp, tn, fp, fn = calc_TP_TN_FP_FN(ytrue_N, yproba1_N >= thresh)\n",
    "    ## Compute ACC, TPR, TNR, etc.\n",
    "    acc = (tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "    tpr = tp / float(tp + fn + 1e-10)\n",
    "    tnr = tn / float(fp + tn + 1e-10)\n",
    "    ppv = tp / float(tp + fp + 1e-10)\n",
    "    npv = tn / float(tn + fn + 1e-10)\n",
    "    \n",
    "    return acc, tpr, tnr, ppv, npv\n",
    "\n",
    "def print_perf_metrics_for_threshold(ytrue_N, yproba1_N, thresh):\n",
    "    ''' Pretty print perf. metrics for a given probabilistic classifier and threshold\n",
    "    '''\n",
    "    acc, tpr, tnr, ppv, npv = calc_perf_metrics_for_threshold(ytrue_N, yproba1_N, thresh)\n",
    "    \n",
    "    ## Pretty print the results\n",
    "    print(\"%.3f ACC\" % acc)\n",
    "    print(\"%.3f TPR\" % tpr)\n",
    "    print(\"%.3f TNR\" % tnr)\n",
    "    print(\"%.3f PPV\" % ppv)\n",
    "    print(\"%.3f NPV\" % npv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_confusion_matrix_for_threshold(ytrue_N, yproba1_N, thresh):\n",
    "    ''' Compute the confusion matrix for a given probabilistic classifier and threshold\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    ytrue_N : 1D array of floats\n",
    "        Each entry represents the binary value (0 or 1) of 'true' label of one example\n",
    "        One entry per example in current dataset\n",
    "    yproba1_N : 1D array of floats\n",
    "        Each entry represents a probability (between 0 and 1) that correct label is positive (1)\n",
    "        One entry per example in current dataset\n",
    "        Needs to be same size as ytrue_N\n",
    "    thresh : float\n",
    "        Scalar threshold for converting probabilities into hard decisions\n",
    "        Calls an example \"positive\" if yproba1 >= thresh\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cm_df : Pandas DataFrame\n",
    "        Can be printed like print(cm_df) to easily display results\n",
    "    '''\n",
    "    cm = sklearn.metrics.confusion_matrix(ytrue_N, yproba1_N >= thresh)\n",
    "    cm_df = pd.DataFrame(data=cm, columns=[0, 1], index=[0, 1])\n",
    "    cm_df.columns.name = 'Predicted'\n",
    "    cm_df.index.name = 'True'\n",
    "    return cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perf_metrics_across_thresholds(ytrue_N, yproba1_N, thresh_grid=None):\n",
    "    ''' Compute common binary classifier performance metrics across many thresholds\n",
    "    \n",
    "    If no array of thresholds is provided, will use all 'unique' values\n",
    "    in the yproba1_N array to define all possible thresholds with different performance.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    ytrue_N : 1D array of floats\n",
    "        Each entry represents the binary value (0 or 1) of 'true' label of one example\n",
    "        One entry per example in current dataset\n",
    "    yproba1_N : 1D array of floats\n",
    "        Each entry represents a probability (between 0 and 1) that correct label is positive (1)\n",
    "        One entry per example in current dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    thresh_grid : 1D array of floats\n",
    "        One entry for each possible threshold\n",
    "    perf_dict : dict, with key, value pairs:\n",
    "        * 'acc' : 1D array of accuracy values (one per threshold)\n",
    "        * 'ppv' : 1D array of positive predictive values (one per threshold)\n",
    "        * 'npv' : 1D array of negative predictive values (one per threshold)\n",
    "        * 'tpr' : 1D array of true positive rates (one per threshold)\n",
    "        * 'tnr' : 1D array of true negative rates (one per threshold)\n",
    "    '''\n",
    "    if thresh_grid is None:\n",
    "        bin_edges = np.linspace(0, 1.001, 21)\n",
    "        thresh_grid = np.sort(np.hstack([bin_edges, np.unique(yproba1_N)]))\n",
    "    tpr_grid = np.zeros_like(thresh_grid)\n",
    "    tnr_grid = np.zeros_like(thresh_grid)\n",
    "    ppv_grid = np.zeros_like(thresh_grid)\n",
    "    npv_grid = np.zeros_like(thresh_grid)\n",
    "    acc_grid = np.zeros_like(thresh_grid)\n",
    "    for tt, thresh in enumerate(thresh_grid):\n",
    "        # Apply specific threshold to convert probas into hard binary values (0 or 1)\n",
    "        # Then count number of true positives, true negatives, etc.\n",
    "        # Then compute metrics like accuracy and true positive rate\n",
    "        acc, tpr, tnr, ppv, npv = calc_perf_metrics_for_threshold(ytrue_N, yproba1_N, thresh)\n",
    "        acc_grid[tt] = acc\n",
    "        tpr_grid[tt] = tpr\n",
    "        tnr_grid[tt] = tnr\n",
    "        ppv_grid[tt] = ppv\n",
    "        npv_grid[tt] = npv\n",
    "    return thresh_grid, dict(\n",
    "        acc=acc_grid,\n",
    "        tpr=tpr_grid,\n",
    "        tnr=tnr_grid,\n",
    "        ppv=ppv_grid,\n",
    "        npv=npv_grid)\n",
    "\n",
    "def make_plot_perf_vs_threshold(ytrue_N, yproba1_N, bin_edges=np.linspace(0, 1, 21)):\n",
    "    ''' Make pretty plot of binary classifier performance as threshold increases\n",
    "    \n",
    "    Produces a plot with 3 rows:\n",
    "    * top row: hist of predicted probabilities for negative examples (shaded red)\n",
    "    * middle row: hist of predicted probabilities for positive examples (shaded blue)\n",
    "    * bottom row: line plots of metrics that require hard decisions (ACC, TPR, TNR, etc.)\n",
    "    '''\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(12, 8))\n",
    "    sns.distplot(\n",
    "        yproba1_N[ytrue_N == 0],\n",
    "        color='r', bins=bin_edges, kde=False, rug=True, ax=axes[0]);\n",
    "    sns.distplot(\n",
    "        yproba1_N[ytrue_N == 1],\n",
    "        color='b', bins=bin_edges, kde=False, rug=True, ax=axes[1]);\n",
    "\n",
    "    thresh_grid, perf_grid = compute_perf_metrics_across_thresholds(ytrue_N, yproba1_N)\n",
    "    axes[2].plot(thresh_grid, perf_grid['acc'], 'k-', label='accuracy')\n",
    "    axes[2].plot(thresh_grid, perf_grid['tpr'], 'b-', label='TPR (recall/sensitivity)')\n",
    "    axes[2].plot(thresh_grid, perf_grid['tnr'], 'g-', label='TNR (specificity)')\n",
    "    axes[2].plot(thresh_grid, perf_grid['ppv'], 'c-', label='PPV (precision)')\n",
    "    axes[2].plot(thresh_grid, perf_grid['npv'], 'm-', label='NPV')\n",
    "    \n",
    "    axes[2].legend()\n",
    "    axes[2].set_ylim([0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Binary Classifier for Cancer-Risk Screening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 3 feature version of x arrays\n",
    "x_tr_M3 = np.loadtxt('data_cancer/x_train.csv', delimiter=',', skiprows=1)\n",
    "x_va_N3 = np.loadtxt('data_cancer/x_valid.csv', delimiter=',', skiprows=1)\n",
    "x_te_N3 = np.loadtxt('data_cancer/x_test.csv', delimiter=',', skiprows=1)\n",
    "\n",
    "# 2 feature version of x arrays\n",
    "x_tr_M2 = x_tr_M3[:, :2].copy()\n",
    "x_va_N2 = x_va_N3[:, :2].copy()\n",
    "x_te_N2 = x_te_N3[:, :2].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr_M = np.loadtxt('data_cancer/y_train.csv', delimiter=',', skiprows=1)\n",
    "y_va_N = np.loadtxt('data_cancer/y_valid.csv', delimiter=',', skiprows=1)\n",
    "y_te_N = np.loadtxt('data_cancer/y_test.csv', delimiter=',', skiprows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1a: Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1a(i):** What fraction of the provided patients have cancer in the training set, the validation set, and the test set? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frac has_cancer on TRAIN: 0.000\n",
      "frac has_cancer on VALID: 0.000\n",
      "frac has_cancer on TEST : 0.000\n"
     ]
    }
   ],
   "source": [
    "print(\"frac has_cancer on TRAIN: %.3f\" % 0.0) # TODO edit the printed values\n",
    "print(\"frac has_cancer on VALID: %.3f\" % 0.0)\n",
    "print(\"frac has_cancer on TEST : %.3f\" % 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1a(ii):** Looking at the features data contained in the training set 𝑥 array, what feature preprocessing (if any) would you recommend to improve a decision tree's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a(iii): Looking at the features data contained in the training set 𝑥 array, what feature preprocessing (if any) would you recommend to improve logistic regression's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1b: The predict-0-always baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1b(i): Compute the accuracy of the predict-0-always classifier on validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc on VALID: 0.000\n",
      "acc on TEST : 0.000\n"
     ]
    }
   ],
   "source": [
    "print(\"acc on VALID: %.3f\" % 0.0) # TODO edit values!\n",
    "print(\"acc on TEST : %.3f\" % 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1b(ii): Print a confusion matrix for predict-0-always on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO call print(calc_confusion_matrix_for_threshold(...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1b(iii): This classifier gets pretty good accuracy! Why wouldn't we want to use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1b(iv): For the intended application (screening patients before biopsy), describe the possible mistakes the classifier can make in task-specific terms. What costs does each mistake entail (lost time? lost money? life-threatening harm?). How do you recommend evaluating the classifier to be mindful of these costs?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1c : Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting for 1c(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_grid = np.logspace(-9, 6, 31)\n",
    "\n",
    "tr_loss_list = list()\n",
    "va_loss_list = list()\n",
    "for C in C_grid:\n",
    "    lr = sklearn.linear_model.LogisticRegression(C=C)\n",
    "\n",
    "    # TODO fit, predict_proba, and evaluate logistic loss\n",
    "    \n",
    "# Record the best model here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1c(i):** Apply your logistic regression code to the \"2 feature\" $x$ data, and make a plot of logistic loss (y-axis) vs. C (x-axis) on the training set and validation set. Which value of $C$ do you prefer? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best C for LR with 2 feature data: 0.000\n"
     ]
    }
   ],
   "source": [
    "# TODO make plot\n",
    "plt.xlabel('log10(C)');\n",
    "plt.ylabel('logistic loss');\n",
    "plt.ylim([0.0, 0.7]);\n",
    "\n",
    "# TODO add legend\n",
    "#plt.legend(...);\n",
    "\n",
    "print(\"best C for LR with 2 feature data: %.3f\" % 0.0) # TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1c(ii):** Make a performance plot that shows how good your probabilistic predictions from the best 1c(i) classifier are on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO call make_plot_perf_vs_threshold(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fitting for 1c(iii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO like 1c(i) but with 3 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1c(iii):** Plot of logistic loss (y-axis) vs. C (x-axis) on the training set and validation set. Which value of $C$ do you prefer? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best C for LR with 3 feature data: 0.000\n"
     ]
    }
   ],
   "source": [
    "# TODO make plot\n",
    "plt.xlabel('log10(C)');\n",
    "plt.ylabel('logistic loss');\n",
    "plt.ylim([0.0, 0.7]);\n",
    "\n",
    "# TODO add legend\n",
    "#plt.legend(...);\n",
    "\n",
    "print(\"best C for LR with 3 feature data: %.3f\" % 0.0) # TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1c(iv):  Make a performance plot that shows how good your probabilistic predictions from the best 1c(iii) classifier are on the validation set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO call make_plot_perf_vs_threshold(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1d: Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fitting code for decision tree 1d(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_leaf_grid = np.asarray([1, 2, 5, 10, 20, 50, 100, 200, y_tr_M.size])\n",
    "\n",
    "tr_loss_list = list()\n",
    "va_loss_list = list()\n",
    "for min_samples_leaf in min_samples_leaf_grid:\n",
    "    tree = sklearn.tree.DecisionTreeClassifier(\n",
    "        criterion='entropy', min_samples_leaf=min_samples_leaf)\n",
    "\n",
    "    # TODO fit, predict_proba, and compute logistic loss\n",
    "\n",
    "# TODO compute best value for min_samples_leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1d(i):** Plot of logistic loss (y-axis) vs. min_samples_leaf (x-axis) on the training set and validation set. Which value of min_samples_leaf do you prefer? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best min_samples_leaf with 3 feature data: 0.000\n"
     ]
    }
   ],
   "source": [
    "# TODO plot\n",
    "plt.xlabel('min_samples_leaf');\n",
    "plt.ylabel('logistic loss');\n",
    "plt.ylim([0.0, 1.0]);\n",
    "\n",
    "print(\"best min_samples_leaf with 3 feature data: %.3f\" % 0.0) # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **1d(ii):** Make a performance plot that shows how good your probabilistic predictions from the best 1c(iii) classifier are on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO call make_plot_perf_vs_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1e: ROC Curve analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1e(i): ROC on Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO something like: fpr, tpr, thr = sklearn.metrics.roc_curve(...)\n",
    "\n",
    "plt.ylim([0, 1]);\n",
    "plt.xlabel(\"FPR\");\n",
    "plt.ylabel(\"TPR\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1e(ii): ROC on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO something like: fpr, tpr, thr = sklearn.metrics.roc_curve(...)\n",
    "\n",
    "plt.ylim([0, 1]);\n",
    "plt.xlabel(\"FPR\");\n",
    "plt.ylabel(\"TPR\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1e(iii):** Short Answer: Compare the 3-feature LR to 2-feature LR models: does one dominate the other in terms of ROC performance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1e(iv):** Short Answer: Compare the 3-feature DTree to 2-feature LR models: does one dominate the other in terms of ROC performance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1f: Selecting a decision threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1f(i): Use default 0.5 threshold. Report perf. for 3-feature Logistic Regr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ON THE VALIDATION SET:\n",
      "Chosen best thr = 0.5000\n",
      "\n",
      "ON THE TEST SET:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_thr = 0.5\n",
    "\n",
    "\n",
    "print(\"ON THE VALIDATION SET:\")\n",
    "print(\"Chosen best thr = %.4f\" % best_thr)\n",
    "print(\"\")\n",
    "print(\"ON THE TEST SET:\")\n",
    "# TODO: print(calc_confusion_matrix_for_threshold(...))\n",
    "print(\"\")\n",
    "# TODO: print(print_perf_metrics_for_threshold(...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1f(ii): Pick threshold to maximize TPR s.t. PPV >= 0.98. Report perf. for 3-feature Logistic Regr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1f(iii): Pick threshold to maximize PPV s.t. TPR >= 0.98. Report perf. for 3-feature Logistic Regr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ON THE VALIDATION SET:\n",
      "Chosen best thr = 0.0000\n",
      "\n",
      "ON THE TEST SET:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO thresh_grid, perf_grid = compute_perf_metrics_across_thresholds(...)\n",
    "\n",
    "# TODO Find threshold that makes TPR as large as possible, while satisfying PPV >= 0.98\n",
    "\n",
    "\n",
    "print(\"ON THE VALIDATION SET:\")\n",
    "print(\"Chosen best thr = %.4f\" % 0.0) # TODO\n",
    "print(\"\")\n",
    "print(\"ON THE TEST SET:\")\n",
    "# TODO: print(calc_confusion_matrix_for_threshold(...))\n",
    "print(\"\")\n",
    "# TODO: print(print_perf_metrics_for_threshold(...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1f(iv): Compare the confusion matrices between 1f(i) - 1f(iii). Which thresholding strategy best meets our preferences from 1a: avoid life-threatening mistakes at all costs, while also eliminating unnecessary biopsies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1f(v): How many subjects in the test set are saved from unnecessary biopsies using your selected thresholding strategy? What fraction of current biopsies would be avoided if this classifier was adopted by the hospital?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Concept Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2a: Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a(i): Where is the ideal minimum of the function $f(x)$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a(ii): Does this gradient descent procedure converge? Explain your answer.M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2a(iii): Can you propose a step length with which the optimization procedure converges?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2b: Understanding Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b(i): Explain why the illustration has problems (1-3 sentences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "7a3fd3d8000e511c2f264f960d6d083f9fff204068dda31fb317937154f24c16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
